# 关于hash
# 问题 有一个 大文件 里面有40亿个数 （可能相同可能不同 假设有K种） 这些数都是32位无符号数（4 byte）
# 给你1G 内存 统计哪个数出现的次数最多
# 很容易想到hash表 但是如果hash表 如果k就是40亿个数 每个都不同 然换一个记录 要有key value 至少要4+4 = 8 byte吧
# 40亿*8 = 320亿 =32 G > 1G 所以不行
# 怎么做 用hash函数
# 先把这个数 过hash函数 然后得到hash code 再 hashcode % 100 -> 把数整到0-99的范围 如果出来时0 进0号文件
# 这样就整出100个文件 对于每个文件内部 只有K/100种数  然后对每个文件 进行hash表统计 统计完记录最大值 释放这1G内存 然后统计下一个文件
# 最后对100个文件的最大值再取最大

# hash表的操作 增删改查 其实是O（logn） 但 python这些语言 用有序树 就是O（1） 所以做题的时候认为是O（1）


# 关于布隆过滤器 是一个很长的二进制向量和一系列随机映射函数。 布隆过滤器可以用于检索一个元素是否在一个集合中。
# 不支持一个东西在集合中删掉
# 比如做一个搜索引擎 或者 有一个大爬虫 有一个大文件 里面有100亿条url 单条url 64byte 我们要去重
# 如果用hashset 要64 * 100亿 = 6400亿byte = 64G
# 如果用上面的hash分流 硬盘读写 空间可以节省 但是时间慢
# 这就要用到布隆过滤器 他是一个位图 但一定会产生失误率
# 假设这个位图长度为m 对于一个url来说 有k个相互独立的不同的hash函数 让url分别过这k个hash 出来的hash code % m 然后把位图那一位变成1
# 经过这k次 改变位图 把0改1  然后如果有两次 %m之后的值一样 还是1 只有0变成1 没有1变成0
# 然后再下一个url 还是对同一个位图 进行一波改动描黑 如果m足够长 结果经过100亿次之后 这个位图有一部分就被描黑了 一部分没描黑
# 所以很简单 如果你要查一个url 就去再过k个hash 然后看结果是不是都在位图上被描黑了 如果是 那就说存在（所以可能失误）但如果没描黑 一定不存在
# 这就是布隆过滤器 而且布隆过滤器大小 只和样本个数有关 和样本大小无关 这里是64byte 就算变了 布隆过滤器大小也不变 只要这个64byte能过hash就好
# 那么m要取多大 k要取多大呢
# 这要看你有多少数据N = 100亿 和你的失误率 假设P= 0.0001 万分之一的失误率
# 如果位图小 m小 失误率必然上升 如果位图很大 可能会有浪费 所以m的大小决定了失误率 有一个公式 m = - (n*ln p) / ((ln 2)^2) 在这道题大约是26G
# 然后算K = ln 2 * m/n 大概是13个
# 比如hadoop里 就有可以布隆过滤器查找 5个文件 哪个有 再到文件里找 就会很快

# 详解一致性哈希原理
#